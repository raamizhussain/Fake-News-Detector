import pandas as pd

# Load the CSVs
fake_df = pd.read_csv("../data/Fake.csv")
real_df = pd.read_csv("../data/True.csv")

# Add labels: fake = 0, real = 1
fake_df["label"] = 0
real_df["label"] = 1

# Merge them
df = pd.concat([fake_df, real_df], ignore_index=True)

# Shuffle the data
df = df.sample(frac=1, random_state=42).reset_index(drop=True)

# Show top rows
df.head()



import re
import string
import nltk
from nltk.corpus import stopwords

# Download stopwords if not already
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

def clean_text(text):
    # Lowercase
    text = text.lower()

    # Remove URLs
    text = re.sub(r"http\S+|www\S+|https\S+", '', text)

    # Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))

    # Remove numbers
    text = re.sub(r'\d+', '', text)

    # Remove stopwords
    text = ' '.join(word for word in text.split() if word not in stop_words)

    # Remove extra spaces
    text = re.sub(r'\s+', ' ', text).strip()

    return text



# Only clean the 'text' column
df['clean_text'] = df['text'].apply(clean_text)

# Preview cleaned data
df[['text', 'clean_text']].head()



get_ipython().getoutput("pip install wordcloud")



import matplotlib.pyplot as plt
from wordcloud import WordCloud
import seaborn as sns

# CLASS BALANCE
sns.countplot(x='label', data=df)
plt.title("Fake vs Real News Count (0 = Fake, 1 = Real)")
plt.show()

# TEXT LENGTH DISTRIBUTION
df['text_length'] = df['clean_text'].apply(lambda x: len(x.split()))
sns.histplot(df['text_length'], kde=True, bins=50)
plt.title("Article Length Distribution")
plt.xlabel("Number of Words")
plt.show()

# WORDCLOUDS
def show_wordcloud(data, title):
    text = ' '.join(data)
    wc = WordCloud(width=800, height=400, background_color='black').generate(text)
    plt.figure(figsize=(10, 5))
    plt.imshow(wc, interpolation='bilinear')
    plt.axis('off')
    plt.title(title)
    plt.show()

# WordCloud for FAKE NEWS
show_wordcloud(df[df.label == 0]['clean_text'], "Fake News - Common Words")

# WordCloud for REAL NEWS
show_wordcloud(df[df.label == 1]['clean_text'], "Real News - Common Words")



from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer

# Extract features and labels
X = df['clean_text']
y = df['label']

# Convert text to TF-IDF vectors
vectorizer = TfidfVectorizer(max_features=5000)
X_vectors = vectorizer.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_vectors, y, test_size=0.2, random_state=42)

print(f"Train shape: {X_train.shape}")
print(f"Test shape: {X_test.shape}")



from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Train model
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

# Accuracy
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")

# Classification Report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=["Fake", "Real"], yticklabels=["Fake", "Real"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()



import pickle

# Save model
with open(r"G:\Learning\fake_news_detector\models\model_text.pkl", "wb") as f:
    pickle.dump(model, f)

# Save vectorizer
with open(r"G:\Learning\fake_news_detector\models\vectorizer_text.pkl", "wb") as f:
    pickle.dump(vectorizer, f)

print("âœ… Model and vectorizer saved.")




